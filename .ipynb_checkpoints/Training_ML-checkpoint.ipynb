{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\"\"\"\n",
    "    Used for data manipulation and analysis\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\"\"\"\n",
    "    It is a suite of libraries and programs for symbolic and statistical natural language \n",
    "    rocessing (NLP) for English written in the Python programming language. \n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\"\"\"\n",
    "    A regular expression (or RE) specifies a set of strings that matches it. The functions \n",
    "    in this module is to check if a particular string matches a given regular expression.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "    Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. \n",
    "    It provides a selection of efficient tools for machine learning and statistical modeling including\n",
    "    classification, regression, clustering and dimensionality reduction via a consistence interface in Python. \n",
    "    This library, which is largely written in Python, is built upon NumPy, SciPy and Matplotlib.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\"\"\"\n",
    "    CountVectorizer transforms text into a sparse matrix of n-gram counts. \n",
    "    Tfidftransformer will systematically compute word counts using CountVectorizer and \n",
    "    then compute the Inverse Document Frequency (IDF) values and only then compute the Tf-idf scores.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\"\"\"\n",
    "    classification_report builds a text report showing the main classification metrics. \n",
    "    confusion_matrix computes confusion matrix to evaluate the accuracy of a classification.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\"\"\"\n",
    "    accuracy_score used to calculate the accuracy of either the faction or count of correct prediction in Python Scikit learn.\n",
    "    Compute the F1 score, also known as balanced F-score or F-measure.\n",
    "    \n",
    "    The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. \n",
    "    The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "    The best value is 1 and the worst value is 0.\n",
    "    \n",
    "    The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. \n",
    "    The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "    The best value is 1 and the worst value is 0.\n",
    "    \n",
    "    The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches \n",
    "    its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.\n",
    "    \n",
    "    This metric is calculated as:\n",
    "    F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\"\"\"\n",
    "    Linear Support Vector Classification is similar to SVC with parameter kernel=’linear’, but implemented in terms of \n",
    "    liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and \n",
    "    should scale better to large numbers of samples.\n",
    "    \n",
    "    This class supports both dense and sparse input and the multiclass support is \n",
    "    handled according to a one-vs-the-rest scheme.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\"\"\"\n",
    "    The pipeline is a Python scikit-learn utility for orchestrating machine learning operations. Pipelines function by \n",
    "    allowing a linear series of data transforms to be linked together, resulting in a measurable modeling process.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\"\"\"\n",
    "    LogisticRegression implements regularized logistic regression using the ‘liblinear’ library, ‘newton-cg’, ‘sag’, \n",
    "    ‘saga’ and ‘lbfgs’ solvers. Regularization is applied by default. It can handle both dense and \n",
    "    sparse input.\n",
    "    \n",
    "    SGDClassifier implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient \n",
    "    of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength \n",
    "    schedule (aka learning rate).\n",
    "    \n",
    "    For best results using the default learning rate schedule, the data should have zero mean and unit variance.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\"\"\"\n",
    "    Multi-layer Perceptron classifier. This model optimizes the log-loss function using LBFGS or \n",
    "    stochastic gradient descent.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\"\"\"\n",
    "    GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary \n",
    "    differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of \n",
    "    the binomial or multinomial deviance loss function. Binary classification is a special case where only a single \n",
    "    regression tree is induced.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn import tree\n",
    "\"\"\"\n",
    "    Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. \n",
    "    The goal is to create a model that predicts the value of a target variable by learning simple decision rules \n",
    "    inferred from the data features. A tree can be seen as a piecewise constant approximation.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "    NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays \n",
    "    and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "    Itertools is a module in python, it is used to iterate over data structures that can be stepped over using a \n",
    "    for-loop. Such data structures are also known as iterables.\n",
    "\"\"\"\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    to plot a sklearn confusion matrix(cm)\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    \"\"\"\n",
    "        Accuracy (all correct / all) = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \"\"\"\n",
    "    \n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('YlOrRd') \n",
    "    \"\"\" \n",
    "        To select the colour theme of the confusion matrix\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    \"\"\"\n",
    "        To create a figure with the given width, height in inches.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    \"\"\"\n",
    "       To display data as an image, i.e., on a 2D regular raster.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \"\"\"\n",
    "       To display a title and colorbar on the axes.\n",
    "    \"\"\"\n",
    "    \n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "        \n",
    "    \"\"\"\n",
    "        To put the labels on the confusion matrix, with or without rotation.\n",
    "    \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \"\"\"\n",
    "        To normalize the confusion matrix by slicing and adding a new axis.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    \"\"\"\n",
    "        To calculate the threshold by finding the maximum value in confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    \"\"\"\n",
    "        To display the values in the confusion matrix with different colours and precision.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \"\"\"\n",
    "        This automatically adjusts subplot params so that the subplot(s) fits in to the figure area. \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \"\"\"\n",
    "        To plot the the labels on their respective axes.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "        To show the confusion matix on screen.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "\"\"\"\n",
    "    Stopwords are the English words which does not add much meaning to a sentence.\n",
    "    They can safely be ignored without sacrificing the meaning of the sentence.\n",
    "\"\"\"\n",
    "def cleantext(string):\n",
    "    text = string.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"http(\\S)+\",' ',text)    \n",
    "    text = re.sub(r\"www(\\S)+\",' ',text)\n",
    "    text = re.sub(r\"&\",' and ',text)  \n",
    "    tx = text.replace('&amp',' ')\n",
    "    text = re.sub(r\"[^0-9a-zA-Z]+\",' ',text)\n",
    "    text = text.split()\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "    \"\"\"\n",
    "        We store the argument of cleantext function in a tuple\n",
    "        variable 'text' and split the words and then join them \n",
    "        after adding the blank spaces between the words.\n",
    "        It returns the string passed after removing all the stopwords.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-d753cd8e9a9a>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-d753cd8e9a9a>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_excel('./Constraint_Train.xlsx')\n",
    "val = pd.read_excel('./Constraint_Val.xlsx')\n",
    "\"\"\"\n",
    "    Read an Excel file into a pandas DataFrame\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"tweet\"] = train[\"tweet\"].map(lambda x: cleantext(x))\n",
    "val[\"tweet\"] = val[\"tweet\"].map(lambda x: cleantext(x))\n",
    "\"\"\"\n",
    "    A lambda function is a small anonymous function that can \n",
    "    take any number of arguments, but can only have one expression.\n",
    "\n",
    "    The map() function executes a specified function for each item in \n",
    "    an iterable. The item is sent to the function as a parameter.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrices(pred,true):\n",
    "    print(confusion_matrix(true,pred))\n",
    "    print(classification_report(true,pred,))\n",
    "    print(\"Accuracy : \",accuracy_score(pred,true))\n",
    "    print(\"Precison : \",precision_score(pred,true, average = 'weighted'))\n",
    "    print(\"Recall : \",recall_score(pred,true,  average = 'weighted'))\n",
    "    print(\"F1 : \",f1_score(pred,true,  average = 'weighted'))\n",
    "    \"\"\"\n",
    "        Here, we are printing the confusion matrix, its accuracy, precision, recall and F1 score.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('bow', CountVectorizer()),  \n",
    "        ('tfidf', TfidfTransformer()),  \n",
    "        ('c', LinearSVC())\n",
    "    ])\n",
    "\"\"\"\n",
    "    The purpose of the pipeline is to assemble several steps that can be \n",
    "    cross-validated together while setting different parameters.\n",
    "\"\"\"\n",
    "fit = pipeline.fit(train['tweet'],train['label'])\n",
    "\"\"\"\n",
    "    “fit” to learn on the data and acquire state \"predict\" to actually \n",
    "    process the data and generate a prediction.\n",
    "\"\"\"\n",
    "print('SVM')\n",
    "print ('val:')\n",
    "pred=pipeline.predict(val['tweet'])\n",
    "\"\"\"\n",
    "    Transform the data, and apply predict with the final estimator.\n",
    "\"\"\"\n",
    "print_metrices(pred,val['label'])\n",
    "create_confusion_matrix(confusion_matrix(val['label'],pred),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of SVM on val data')\n",
    "\n",
    "val_ori = pd.read_excel('./Constraint_Val.xlsx')\n",
    "svm_val_misclass_df = val_ori[pred!=val['label']]\n",
    "\"\"\"\n",
    "    Transform the data from the excel sheet which are not under 'label' \n",
    "    column and apply predict with the final estimator.\n",
    "\n",
    "    This will be used later to print additional informations, like data types and memory used.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('bow', CountVectorizer()),  \n",
    "        ('tfidf', TfidfTransformer()),  \n",
    "        ('c', LogisticRegression())\n",
    "    ])\n",
    "fit = pipeline.fit(train['tweet'],train['label'])\n",
    "print('Logistic Regression')\n",
    "print ('val:')\n",
    "pred=pipeline.predict(val['tweet'])\n",
    "\n",
    "print_metrices(pred,val['label'])\n",
    "create_confusion_matrix(confusion_matrix(val['label'],pred),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of LR on val data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('bow', CountVectorizer()),  \n",
    "        ('tfidf', TfidfTransformer()),  \n",
    "        ('c', GradientBoostingClassifier())\n",
    "    ])\n",
    "fit = pipeline.fit(train['tweet'],train['label'])\n",
    "print('Gradient Boost')\n",
    "print ('val:')\n",
    "pred=pipeline.predict(val['tweet'])\n",
    "\n",
    "print_metrices(pred,val['label'])\n",
    "create_confusion_matrix(confusion_matrix(val['label'],pred),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of GDBT on val data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('bow', CountVectorizer()),  \n",
    "        ('tfidf', TfidfTransformer()),  \n",
    "        ('c', tree.DecisionTreeClassifier())\n",
    "    ])\n",
    "fit = pipeline.fit(train['tweet'],train['label'])\n",
    "print('Decision Tree')\n",
    "print ('val:')\n",
    "pred=pipeline.predict(val['tweet'])\n",
    "\n",
    "print_metrices(pred,val['label'])\n",
    "create_confusion_matrix(confusion_matrix(val['label'],pred),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of DT on val data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_val_misclass_df.info()\n",
    "\"\"\"\n",
    "    Here we print additional informations, like data types and memory used.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_val_misclass_df.to_excel('svm_val_misclassified.xlsx')\n",
    "\"\"\"\n",
    "    Here we store the final results to an excel file.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
