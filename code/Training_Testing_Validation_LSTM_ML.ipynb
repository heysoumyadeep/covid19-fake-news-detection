{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\"\"\"\n",
    "    Used for data manipulation and analysis\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. \n",
    "\"\"\"\n",
    "\n",
    "import itertools\n",
    "\"\"\" \n",
    "    Itertools is a module in python, it is used to iterate over data structures that can be stepped over using a \n",
    "    for-loop. Such data structures are also known as iterables.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\"\"\"\n",
    "    classification_report builds a text report showing the main classification metrics. \n",
    "    confusion_matrix computes confusion matrix to evaluate the accuracy of a classification.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    accuracy_score used to calculate the accuracy of either the faction or count of correct prediction in Python Scikit learn.\n",
    "    Compute the F1 score, also known as balanced F-score or F-measure.\n",
    "    \n",
    "    The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. \n",
    "    The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "    The best value is 1 and the worst value is 0.\n",
    "    \n",
    "    The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. \n",
    "    The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "    The best value is 1 and the worst value is 0.\n",
    "    \n",
    "    The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches \n",
    "    its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.\n",
    "    \n",
    "    This metric is calculated as:\n",
    "    F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\"\"\"\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Read an Excel file into a pandas DataFrame\n",
    "\"\"\"\n",
    "df=pd.read_excel('../data/Constraint_Train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>States reported 1121 deaths a small rise from ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Populous states can generate large case counts...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              tweet label\n",
       "0  1.0  The CDC currently reports 99031 deaths. In gen...  real\n",
       "1  2.0  States reported 1121 deaths a small rise from ...  real\n",
       "2  3.0  Politically Correct Woman (Almost) Uses Pandem...  fake\n",
       "3  4.0  #IndiaFightsCorona: We have 1524 #COVID testin...  real\n",
       "4  5.0  Populous states can generate large case counts...  real"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Prints top 5 rows of excel file\n",
    "\"\"\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Drops the rows containing Null values\n",
    "\"\"\"\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Drops the independent features, and stores it into X\n",
    "\"\"\"\n",
    "X=df.drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['label']\n",
    "\"\"\"\n",
    "    Drops the dependent features, and stores it into y\n",
    "\"\"\"\n",
    "type(y)\n",
    "arr=[]\n",
    "for i in y:\n",
    "    if i=='real':\n",
    "        arr.append([1])\n",
    "    else:\n",
    "        arr.append([0])\n",
    "\"\"\"\n",
    "    Here, we are converting 'real' and 'fake' to '1' and '0' respectively.\n",
    "\"\"\"\n",
    "y=arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19096/463718857.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \"\"\"\n\u001b[0;32m      3\u001b[0m     \u001b[0mIt\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0macross\u001b[0m \u001b[0ma\u001b[0m \u001b[0mrange\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtasks\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mhas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mparticular\u001b[0m \u001b[0mfocus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mon\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minference\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdeep\u001b[0m \u001b[0mneural\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \"\"\"\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\"\"\"\n",
    "    It can be used across a range of tasks but has a particular focus\n",
    "    on training and inference of deep neural networks.\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\"\"\"\n",
    "    embedding is a dense vector of floating point values\n",
    "    (the length of the vector is a parameter that is specified).\n",
    "\"\"\"\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\"\"\"\n",
    "    To make each input length fixed\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\"\"\"\n",
    "    A Sequential model is appropriate for a plain stack of layers \n",
    "    where each layer has exactly one input tensor and one output tensor.\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "\"\"\"\n",
    "    One-hot encodes a text into a list of word indexes of size n.\n",
    "    It returns a list of encoded integers each corresponding to a word \n",
    "    (or token) in the given input string.\n",
    "\n",
    "\"\"\"\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\"\"\"\n",
    "    Imports LSTM Classificatio model from Keras\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    Dense implements the operation:\n",
    "    output = activation(dot(input, kernel) + bias). \n",
    "    These are all attributes of Dense. \n",
    "\"\"\"\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Specify the vocabulary size, to be used later.\n",
    "\"\"\"\n",
    "voc_size=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Copies the tuple X into variable messages.\n",
    "\"\"\"\n",
    "messages=X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Reset the index, or a level of messages dataframe \n",
    "    to get a new order of arrangement.\n",
    "\"\"\"\n",
    "messages.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\"\"\"\n",
    "    It is a suite of libraries and programs for symbolic and statistical natural language \n",
    "    processing (NLP) for English written in the Python programming language. \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    A regular expression (or RE) specifies a set of strings that matches it. The functions \n",
    "    in this module is to check if a particular string matches a given regular expression.\n",
    "\"\"\"\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preprocessing is done here.\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\"\"\"\n",
    "    Algorithm to for removing the commoner morphological \n",
    "    and inflexional endings from words in English. \n",
    "\"\"\"\n",
    "\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "\n",
    "\"\"\"\n",
    "        In this loop, the words in the dataframe 'messages' is splitted and then joined \n",
    "        after adding the blank spaces between the words and stored in tuple variable 'review'.\n",
    "        Followed by this, all the stopwords are removed.\n",
    "\"\"\"\n",
    "for i in range(0, len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['tweet'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    One-hot encodes a text into a list of word indexes of size n.\n",
    "    It returns a list of encoded integers each corresponding to a word \n",
    "    (or token) in the given input string.\n",
    "\"\"\"\n",
    "onehot_repr=[one_hot(words,voc_size)for words in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length=20\n",
    "\"\"\"\n",
    "    Maximum Sentence Length in the tweet.\n",
    "\"\"\"\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "\"\"\"\n",
    "    Here we are padding before each sequence where the maximum length = sent_length\n",
    "\"\"\"\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "embedding_vector_features=40\n",
    "model=Sequential()\n",
    "\"\"\"\n",
    "    Sequential model is a linear stack of layers.\n",
    "\"\"\"\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "\"\"\"\n",
    "    We're adding different features to the sequential model\n",
    "\"\"\"\n",
    "model.add(LSTM(100))\n",
    "\"\"\"\n",
    "    It is the number of LSTM memory units connected to each input of the series.\n",
    "\"\"\"\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\"\"\"\n",
    "    Here we're are using the layer activation funtion 'sigmoid'.\n",
    "\"\"\"\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\"\"\"\n",
    "    Binary cross entropy compares each of the predicted probabilities to \n",
    "    actual class output which can be either 0 or 1, and use it as a loss funtion.\n",
    "    \n",
    "    Adam optimizer involves a combination of two gradient descent methodologies: \n",
    "        - Momentum.\n",
    "        - Root Mean Square Propagation (RMSP). \n",
    "\"\"\"\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    We're converting the datatype from list to array.\n",
    "\"\"\"\n",
    "X_final=embedded_docs\n",
    "\n",
    "import numpy as np\n",
    "y_final=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\"\"\"\n",
    "    Here we are spliting the dataset, where the test_size is 1/3 of all data and we are initializing the\n",
    "    internal randoms number generator as 42.\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Here we are training the model where epochs is the number of times we iterate over the training set \n",
    "    and the number of training examples utilized in one iteration.\n",
    "\"\"\"\n",
    "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=15,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\"\"\"\n",
    "    Dropout is a technique used to prevent a model from overfitting.\n",
    "\"\"\"\n",
    "model.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The model is fitted with trained data, and then used to make prediction.\n",
    "\"\"\"\n",
    "y_pred=(model.predict(X_test) >= 0.5).astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    to plot a sklearn confusion matrix(cm)\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    \"\"\"\n",
    "        Accuracy (all correct / all) = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \"\"\"\n",
    "    \n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('YlOrRd') \n",
    "    \"\"\" \n",
    "        To select the colour theme of the confusion matrix\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    \"\"\"\n",
    "        To create a figure with the given width, height in inches.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    \"\"\"\n",
    "       To display data as an image, i.e., on a 2D regular raster.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \"\"\"\n",
    "       To display a title and colorbar on the axes.\n",
    "    \"\"\"\n",
    "    \n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "        \n",
    "    \"\"\"\n",
    "        To put the labels on the confusion matrix, with or without rotation.\n",
    "    \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \"\"\"\n",
    "        To normalize the confusion matrix by slicing and adding a new axis.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    \"\"\"\n",
    "        To calculate the threshold by finding the maximum value in confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    \"\"\"\n",
    "        To display the values in the confusion matrix with different colours and precision.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \"\"\"\n",
    "        This automatically adjusts subplot params so that the subplot(s) fits in to the figure area. \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \"\"\"\n",
    "        To plot the the labels on their respective axes.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "        To show the confusion matix on screen.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def print_metrices(y_pred,y_test):\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred,))\n",
    "    print(\"Accuracy : \",accuracy_score(y_pred,y_test))\n",
    "    print(\"Precison : \",precision_score(y_pred,y_test, average = 'weighted'))\n",
    "    print(\"Recall : \",recall_score(y_pred,y_test,  average = 'weighted'))\n",
    "    print(\"F1 : \",f1_score(y_pred,y_test,  average = 'weighted'))\n",
    "    \"\"\"\n",
    "        Here, we are printing the confusion matrix, its accuracy, precision, recall and F1 score.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LSTM')\n",
    "print ('val:')\n",
    "print_metrices(y_pred,y_test)\n",
    "\"\"\"\n",
    "    Transform the data from the excel sheet which are not under 'label' \n",
    "    column and apply predict with the final estimator.\n",
    "\n",
    "    This will be used later to print additional informations, like data types and memory used.\n",
    "\"\"\"\n",
    "create_confusion_matrix(confusion_matrix(y_test,y_pred),target_names=['fake','real'], normalize = False, \\\n",
    "                      title = 'Confusion matix of LSTM on val data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
